{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP8xokhkqNWjjjcXk6v8fZa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alaaibrahim2/Alaa/blob/main/Language_Identifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deYWTkTHtxBT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/dataset.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "ZwO0tcwj8-Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "xryl0rrH_2Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "QbsRBidF__2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "rEE8OpaIAJ_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "YSlkY1U2AcP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.dropna()"
      ],
      "metadata": {
        "id": "WqXMEIowAmB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "S5H-tNnuBpZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['language'].value_counts()"
      ],
      "metadata": {
        "id": "Hf0-ZAzhBsRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(df['Text'])\n",
        "y = np.array(df['language'])"
      ],
      "metadata": {
        "id": "eyMSjlqYB_nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "ILMMzr6XCA48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t0FyKJzJD8ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect = CountVectorizer()"
      ],
      "metadata": {
        "id": "yLV5d13SCE2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = count_vect.fit_transform(x)"
      ],
      "metadata": {
        "id": "QjXB1e-XCIKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=3)"
      ],
      "metadata": {
        "id": "uE_O11IjCMTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WIm-ytuYCPrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "0gD8lbO9CVPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=LogisticRegression()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "mpnBQbozD-q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "DjAwHJKbEJ66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=RandomForestClassifier()\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "SZDVODG5FZhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "PUzPutj-Ix6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D"
      ],
      "metadata": {
        "id": "-rirSdbWJDEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "breakpoint = int(0.8 * len(df))\n",
        "train_x = df['Text'][:breakpoint]\n",
        "test_x = df['Text'][breakpoint:]"
      ],
      "metadata": {
        "id": "0OHv0LU6W7jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ys = df['language']\n"
      ],
      "metadata": {
        "id": "e8EofMMOak-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 40000\n",
        "embedding_dim = 64\n",
        "max_length = 150\n",
        "trunc_type ='post'\n",
        "oov_tok = '<OOV>'\n",
        "num_unique_languages = len(set(ys))"
      ],
      "metadata": {
        "id": "d23-04YKazUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
        "tokenizer.fit_on_texts(train_x)\n",
        "word_index = tokenizer.word_index\n",
        "training_sequences = tokenizer.texts_to_sequences(train_x)\n",
        "training_padded = pad_sequences(training_sequences, maxlen = max_length)"
      ],
      "metadata": {
        "id": "SkrwvhVTcTUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = tokenizer.texts_to_sequences(test_x)\n",
        "test_padded = pad_sequences(test_sequences, maxlen = max_length)"
      ],
      "metadata": {
        "id": "-UKzOTL4csAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(ys)\n",
        "word_index_y = label_tokenizer.word_index\n",
        "label_sequences = label_tokenizer.texts_to_sequences(ys)\n",
        "label_padded = pad_sequences(label_sequences)"
      ],
      "metadata": {
        "id": "nI7RsWtjczU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_padded = np.array(training_padded)\n",
        "test_padded = np.array(test_padded)\n",
        "training_ys = np.squeeze(np.array(label_padded[:breakpoint]))\n",
        "test_ys = np.squeeze(np.array(label_padded[breakpoint:]))\n",
        "training_ys = training_ys - 1\n",
        "test_ys = test_ys - 1\n",
        "final_y_index = dict()\n",
        "for language, value in word_index_y.items():\n",
        "    final_y_index[language] = value - 1\n",
        "word_index_y = final_y_index\n",
        "del(final_y_index)\n",
        "training_ys = to_categorical(training_ys, num_classes=num_unique_languages)\n",
        "test_ys = to_categorical(test_ys, num_classes=num_unique_languages)"
      ],
      "metadata": {
        "id": "sUMbnEvzc7dC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.layers as layers\n",
        "model = Sequential([\n",
        "    layers.Embedding(vocab_size, embedding_dim, input_length=max_length), # hyperparameters set above\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(num_unique_languages, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1JwwUIGKdgJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "history = model.fit(training_padded, training_ys, epochs=num_epochs, validation_data=(test_padded, test_ys), verbose=2)"
      ],
      "metadata": {
        "id": "bqQKRGnxdyVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "UBHvCrF4eocX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, index_to_word_y, max_length, trunc_type):\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "    predictions = model.predict(padded)\n",
        "    for prediction in predictions:\n",
        "        most_likely_lang = np.argmax(prediction)\n",
        "        print(index_to_word_y[most_likely_lang])"
      ],
      "metadata": {
        "id": "MWEEBWJYez5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['Pedro Sánchez Pérez-Castejón (Madrid, 29 de febrero de 1972) es un político español, actual presidente del Gobierno de España.', \n",
        "            'Hallo Hans. Wie geht es heute? Hast du Mittagessen gegessen?', # The model doesn't know German exists!\n",
        "            'Emmanuel Macron, né le 21 décembre 1977 à Amiens, est un haut fonctionnaire, banquier d\\'affaires et homme d\\'État français. Il est président de la République française depuis le 14 mai 2017. ',\n",
        "            'กรุงเทพมหานคร เป็นเมืองหลวงและนครที่มีประชากรมากที่สุดของประเทศไทย เป็นศูนย์กลางการปกครอง การศึกษา การคมนาคมขนส่ง การเงินการธนาคาร การพาณิชย์ การสื่อสาร และความเจริญของประเทศ ตั้งอยู่บนสามเหลี่ยมปากแม่น้ำเจ้าพระยา มีแม่น้ำเจ้าพระยาไหลผ่านและแบ่งเมืองออกเป็น 2 ฝั่ง คือ ฝั่งพระนครและฝั่งธนบุรี กรุงเทพมหานครมีพื้นที่ทั้งหมด 1,568.737 ตร.กม.',\n",
        "            '你好!',\n",
        "            'Gallia  est  omnis  dīvīsa  in  partēs  trēs,  quārum ūnam  incolunt  Belgae,  aliam Aquītānī,  tertiam  quī  ipsōrum  linguā  Celtae,nostrā  Gallī  appellantur.  '\n",
        "           ]\n",
        "index_to_word_y = dict([(value, key) for (key,value) in word_index_y.items()])\n",
        "predict(sentence, tokenizer, index_to_word_y, max_length, trunc_type)"
      ],
      "metadata": {
        "id": "QVZYxgS3e7TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 8192"
      ],
      "metadata": {
        "id": "7Fl3rZFyf6lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim = max_words, output_dim = 256))\n",
        "model_lstm.add(SpatialDropout1D(0.3))\n",
        "model_lstm.add(LSTM(256, dropout = 0.3, recurrent_dropout = 0.3))\n",
        "model_lstm.add(Dense(256, activation = 'relu'))\n",
        "model_lstm.add(Dropout(0.3))\n",
        "model_lstm.add(Dense(5, activation = 'softmax'))\n",
        "model_lstm.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='Adam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "e8hOdlUgfxYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 25\n",
        "history = model.fit(training_padded, training_ys, epochs=num_epochs, validation_data=(test_padded, test_ys), verbose=2)"
      ],
      "metadata": {
        "id": "w_xINsCJgqBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "  \n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "a6bXtnxEhj34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, tokenizer, index_to_word_y, max_length, trunc_type):\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "    predictions = model.predict(padded)\n",
        "    for prediction in predictions:\n",
        "        most_likely_lang = np.argmax(prediction)\n",
        "        print(index_to_word_y[most_likely_lang])"
      ],
      "metadata": {
        "id": "L2Ot56DHhzq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['Pedro Sánchez Pérez-Castejón (Madrid, 29 de febrero de 1972) es un político español, actual presidente del Gobierno de España.', \n",
        "            'Hallo Hans. Wie geht es heute? Hast du Mittagessen gegessen?', # The model doesn't know German exists!\n",
        "            'Emmanuel Macron, né le 21 décembre 1977 à Amiens, est un haut fonctionnaire, banquier d\\'affaires et homme d\\'État français. Il est président de la République française depuis le 14 mai 2017. ',\n",
        "            'กรุงเทพมหานคร เป็นเมืองหลวงและนครที่มีประชากรมากที่สุดของประเทศไทย เป็นศูนย์กลางการปกครอง การศึกษา การคมนาคมขนส่ง การเงินการธนาคาร การพาณิชย์ การสื่อสาร และความเจริญของประเทศ ตั้งอยู่บนสามเหลี่ยมปากแม่น้ำเจ้าพระยา มีแม่น้ำเจ้าพระยาไหลผ่านและแบ่งเมืองออกเป็น 2 ฝั่ง คือ ฝั่งพระนครและฝั่งธนบุรี กรุงเทพมหานครมีพื้นที่ทั้งหมด 1,568.737 ตร.กม.',\n",
        "            '你好!',\n",
        "            'Gallia  est  omnis  dīvīsa  in  partēs  trēs,  quārum ūnam  incolunt  Belgae,  aliam Aquītānī,  tertiam  quī  ipsōrum  linguā  Celtae,nostrā  Gallī  appellantur.  '\n",
        "           ]\n",
        "index_to_word_y = dict([(value, key) for (key,value) in word_index_y.items()])\n",
        "predict(sentence, tokenizer, index_to_word_y, max_length, trunc_type)"
      ],
      "metadata": {
        "id": "FIFkahGch6yI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}