{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXD2fHdXLmJAs+FQ0eRUuA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alaaibrahim2/Alaa/blob/main/CGANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHDDLBxdom5T",
        "outputId": "d17205fa-26a1-4c78-fd19-0d0a6df2518e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Train (60000, 28, 28) (60000,)\n",
            "Test (10000, 28, 28) (10000,)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets.fashion_mnist import load_data\n",
        "# load the images into memory\n",
        "(trainX, trainy), (testX, testy) = load_data()\n",
        "# summarize the shape of the dataset\n",
        "print('Train', trainX.shape, trainy.shape)\n",
        "print('Test', testX.shape, testy.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_discriminator(in_shape=(28,28,1)):\n",
        " model = Sequential()\n",
        " # downsample\n",
        " model.add(Conv2D(128, (3,3), strides=(2,2), padding='same', input_shape=in_shape))\n",
        " model.add(LeakyReLU(alpha=0.2))\n",
        " # downsample\n",
        " model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        " model.add(LeakyReLU(alpha=0.2))\n",
        " # classifier\n",
        " model.add(Flatten())\n",
        " model.add(Dropout(0.4))\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        " # compile model\n",
        " opt = Adam(lr=0.0002, beta_1=0.5)\n",
        " model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        " return model"
      ],
      "metadata": {
        "id": "FGZbGQujoqAn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_generator(latent_dim):\n",
        "\tmodel = Sequential()\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 128 * 7 * 7\n",
        "\tmodel.add(Dense(n_nodes, input_dim=latent_dim))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\tmodel.add(Reshape((7, 7, 128)))\n",
        "\t# upsample to 14x14\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# upsample to 28x28\n",
        "\tmodel.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "\tmodel.add(LeakyReLU(alpha=0.2))\n",
        "\t# generate\n",
        "\tmodel.add(Conv2D(1, (7,7), activation='tanh', padding='same'))\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "_WvbpHUVpP18"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_gan(generator, discriminator):\n",
        " # make weights in the discriminator not trainable\n",
        " discriminator.trainable = False\n",
        " # connect them\n",
        " model = Sequential()\n",
        " # add generator\n",
        " model.add(generator)\n",
        " # add the discriminator\n",
        " model.add(discriminator)\n",
        " # compile model\n",
        " opt = Adam(lr=0.0002, beta_1=0.5)\n",
        " model.compile(loss='binary_crossentropy', optimizer=opt)\n",
        " return model"
      ],
      "metadata": {
        "id": "EqlWUZQ2pYdo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_real_samples():\n",
        " # load dataset\n",
        " (trainX, _), (_, _) = load_data()\n",
        " # expand to 3d, e.g. add channels\n",
        " X = expand_dims(trainX, axis=-1)\n",
        " # convert from ints to floats\n",
        " X = X.astype('float32')\n",
        " # scale from [0,255] to [-1,1]\n",
        " X = (X - 127.5) / 127.5\n",
        " return X"
      ],
      "metadata": {
        "id": "rhpuMPxnpfoA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_real_samples(dataset, n_samples):\n",
        " # choose random instances\n",
        " ix = randint(0, dataset.shape[0], n_samples)\n",
        " # select images\n",
        " X = dataset[ix]\n",
        " # generate class labels\n",
        " y = ones((n_samples, 1))\n",
        " return X, y"
      ],
      "metadata": {
        "id": "dOVUmJzmpktb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\n",
        " # generate points in the latent space\n",
        " x_input = randn(latent_dim * n_samples)\n",
        " # reshape into a batch of inputs for the network\n",
        " x_input = x_input.reshape(n_samples, latent_dim)\n",
        " return x_input"
      ],
      "metadata": {
        "id": "zF59gID9oqnc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        " # generate points in latent space\n",
        " x_input = generate_latent_points(latent_dim, n_samples)\n",
        " # predict outputs\n",
        " X = generator.predict(x_input)\n",
        " # create class labels\n",
        " y = zeros((n_samples, 1))\n",
        " return X, y"
      ],
      "metadata": {
        "id": "5yTtefVFoqpW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n",
        "\tbat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
        "\thalf_batch = int(n_batch / 2)  #the discriminator model is updated for a half batch of real samples \n",
        "                            #and a half batch of fake samples, combined a single batch. \n",
        "\t# manually enumerate epochs\n",
        "\tfor i in range(n_epochs):\n",
        "\t\t# enumerate batches over the training set\n",
        "\t\tfor j in range(bat_per_epo):\n",
        "\t\t\t\n",
        "             # Train the discriminator on real and fake images, separately (half batch each)\n",
        "        #Research showed that separate training is more effective. \n",
        "\t\t\t# get randomly selected 'real' samples\n",
        "            # get randomly selected 'real' samples\n",
        "\t\t\t[X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
        "\n",
        "            # update discriminator model weights\n",
        "            ##train_on_batch allows you to update weights based on a collection \n",
        "            #of samples you provide\n",
        "\t\t\td_loss_real, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
        "            \n",
        "\t\t\t# generate 'fake' examples\n",
        "\t\t\t[X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "\t\t\t# update discriminator model weights\n",
        "\t\t\td_loss_fake, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
        "            \n",
        "            #d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) #Average loss if you want to report single..\n",
        "            \n",
        "\t\t\t# prepare points in latent space as input for the generator\n",
        "\t\t\t[z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n",
        "            \n",
        "        # The generator wants the discriminator to label the generated samples\n",
        "        # as valid (ones)\n",
        "        #This is where the generator is trying to trick discriminator into believing\n",
        "        #the generated image is true (hence value of 1 for y)\t\n",
        "\t\t\t# create inverted labels for the fake samples\n",
        "      y_gan = ones((n_batch, 1))\n",
        "        # Generator is part of combined model where it got directly linked with the discriminator\n",
        "        # Train the generator with latent_dim as x and 1 as y. \n",
        "        # Again, 1 as the output as it is adversarial and if generator did a great\n",
        "        # job of folling the discriminator then the output would be 1 (true)\n",
        "\t\t\t# update the generator via the discriminator's error\n",
        "\t\t\tg_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
        "\t\t\t# Print losses on this batch\n",
        "\t\t\tprint('Epoch>%d, Batch%d/%d, d1=%.3f, d2=%.3f g=%.3f' %\n",
        "\t\t\t\t(i+1, j+1, bat_per_epo, d_loss_real, d_loss_fake, g_loss))\n",
        "\t# save the generator model\n",
        "\tg_model.save('cifar_conditional_generator_25epochs.h5')"
      ],
      "metadata": {
        "id": "4soNcEEcoqrs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 100\n",
        "# create the discriminator\n",
        "d_model = define_discriminator()\n",
        "# create the generator\n",
        "g_model = define_generator(latent_dim)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model)\n",
        "# load image data\n",
        "dataset = load_real_samples()\n",
        "# train model\n",
        "train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=50)\n",
        "     "
      ],
      "metadata": {
        "id": "XdIhxJWkrAN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrvamFnNoquU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5__9aOsvoqxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKmocdDGoqz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymHryzRSoq2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eHS1w1n4oq59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BwHXyvYPoq8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vUNI1dtoq_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7DZUtXfborCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iOGdoV-orFM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}